{"cells":[{"cell_type":"markdown","metadata":{"id":"lqt_yzRy16Wj"},"source":["## Task\n","\n","In this compulsory task you will clean the country column and parse the date column in the **store_income_data_task.csv** file."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"vBP3WN2O16Wp"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>store_name</th>\n","      <th>store_email</th>\n","      <th>department</th>\n","      <th>income</th>\n","      <th>date_measured</th>\n","      <th>country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>FIRST REPUBLIC BANK</td>\n","      <td>ecanadine3@fc2.com</td>\n","      <td>Automotive</td>\n","      <td>$8928350.04</td>\n","      <td>8-5-2006</td>\n","      <td>Britain/</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>Auburn National Bancorporation, Inc.</td>\n","      <td>ccaldeyroux5@dion.ne.jp</td>\n","      <td>Grocery</td>\n","      <td>$69798987.04</td>\n","      <td>19-9-1999</td>\n","      <td>U.K.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>Interlink Electronics, Inc.</td>\n","      <td>orodenborch6@skyrock.com</td>\n","      <td>Garden</td>\n","      <td>$22521052.79</td>\n","      <td>8-6-2001</td>\n","      <td>SA</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>Synopsys, Inc.</td>\n","      <td>lcancellieri9@tmall.com</td>\n","      <td>Electronics</td>\n","      <td>$44091294.62</td>\n","      <td>11-7-2006</td>\n","      <td>United Kingdom</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>New Home Company Inc. (The)</td>\n","      <td>nhinchcliffef@whitehouse.gov</td>\n","      <td>Shoes</td>\n","      <td>$90808764.99</td>\n","      <td>21-4-1993</td>\n","      <td>Britain</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>987</th>\n","      <td>988</td>\n","      <td>Ares Dynamic Credit Allocation Fund, Inc.</td>\n","      <td>fbrocklebankrf@sitemeter.com</td>\n","      <td>Health</td>\n","      <td>$34762898.80</td>\n","      <td>15-12-1989</td>\n","      <td>SA/</td>\n","    </tr>\n","    <tr>\n","      <th>989</th>\n","      <td>990</td>\n","      <td>Madison Covered Call &amp; Equity Strategy Fund</td>\n","      <td>nbaikerh@list-manage.com</td>\n","      <td>Electronics</td>\n","      <td>$85704421.13</td>\n","      <td>16-7-1993</td>\n","      <td>UK.</td>\n","    </tr>\n","    <tr>\n","      <th>991</th>\n","      <td>992</td>\n","      <td>Atlantic Capital Bancshares, Inc.</td>\n","      <td>mgribbellrj@symantec.com</td>\n","      <td>Clothing</td>\n","      <td>$83355366.27</td>\n","      <td>13-4-2001</td>\n","      <td>S.A..</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>996</td>\n","      <td>Columbia Sportswear Company</td>\n","      <td>cschooleyrn@sohu.com</td>\n","      <td>Automotive</td>\n","      <td>$52593924.99</td>\n","      <td>7-10-2005</td>\n","      <td>S. AfricaSouth Africa/</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>998</td>\n","      <td>Tortoise Energy Infrastructure Corporation</td>\n","      <td>cbeardshallrp@ow.ly</td>\n","      <td>Health</td>\n","      <td>$1697293.64</td>\n","      <td>25-4-2009</td>\n","      <td>UNITED STATES OF AMERICA</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>384 rows Ã— 7 columns</p>\n","</div>"],"text/plain":["      id                                   store_name  \\\n","3      4                          FIRST REPUBLIC BANK   \n","5      6         Auburn National Bancorporation, Inc.   \n","6      7                  Interlink Electronics, Inc.   \n","9     10                               Synopsys, Inc.   \n","15    16                  New Home Company Inc. (The)   \n","..   ...                                          ...   \n","987  988    Ares Dynamic Credit Allocation Fund, Inc.   \n","989  990  Madison Covered Call & Equity Strategy Fund   \n","991  992            Atlantic Capital Bancshares, Inc.   \n","995  996                  Columbia Sportswear Company   \n","997  998   Tortoise Energy Infrastructure Corporation   \n","\n","                      store_email   department        income date_measured  \\\n","3              ecanadine3@fc2.com   Automotive   $8928350.04      8-5-2006   \n","5         ccaldeyroux5@dion.ne.jp      Grocery  $69798987.04     19-9-1999   \n","6        orodenborch6@skyrock.com       Garden  $22521052.79      8-6-2001   \n","9         lcancellieri9@tmall.com  Electronics  $44091294.62     11-7-2006   \n","15   nhinchcliffef@whitehouse.gov        Shoes  $90808764.99     21-4-1993   \n","..                            ...          ...           ...           ...   \n","987  fbrocklebankrf@sitemeter.com       Health  $34762898.80    15-12-1989   \n","989      nbaikerh@list-manage.com  Electronics  $85704421.13     16-7-1993   \n","991      mgribbellrj@symantec.com     Clothing  $83355366.27     13-4-2001   \n","995          cschooleyrn@sohu.com   Automotive  $52593924.99     7-10-2005   \n","997           cbeardshallrp@ow.ly       Health   $1697293.64     25-4-2009   \n","\n","                      country  \n","3                    Britain/  \n","5                        U.K.  \n","6                         SA   \n","9              United Kingdom  \n","15                    Britain  \n","..                        ...  \n","987                       SA/  \n","989                       UK.  \n","991                     S.A..  \n","995    S. AfricaSouth Africa/  \n","997  UNITED STATES OF AMERICA  \n","\n","[384 rows x 7 columns]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# Load up store_income_data.csv\n","import pandas as pd\n","import numpy as np\n","\n","\n","income_df = pd.read_csv('store_income_data_task.csv')\n","income_df.dropna()\n"]},{"cell_type":"markdown","metadata":{"id":"ItqLwumA16Wr"},"source":["1. Take a look at all the unique values in the \"country\" column. Then, convert the column to lowercase and remove any trailing white spaces."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"sLkzt4Hr16Wr"},"outputs":[],"source":["#Working out the number of unique countries\n","income_df['country'].nunique()\n","\n","# converting all the entries in the 'country' column to be lower case strings\n","income_df['country']=income_df['country'].str.lower()\n","\n","# removing all trailing white spaces\n","income_df['country']=income_df['country'].str.strip()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P6dcDc4P16Ws"},"source":["2. Note that there should only be three separate countries. Eliminate all variations, so that 'South Africa', 'United Kingdom' and 'United States' are the only three countries."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"qeV3CxMR16Ws"},"outputs":[{"data":{"text/plain":["array(['united states/', 'britain', 'united states', 'britain/',\n","       'united kingdom', 'u.k.', 'sa', 'u.k/', 'america', nan, 's.a.',\n","       'england', 'uk', 's.a./', 'u.k', 'america/', 'sa.', '', 'uk.',\n","       'england/', 'united states of america', 'uk/', 'sa/', 'england.',\n","       'america.', 's.a..', 'united states of america.',\n","       'united states of america/', 'united states.',\n","       's. africasouth africa', 'britain.', '/', 'united kingdom.',\n","       's. africasouth africa/', 'united kingdom/',\n","       's. africasouth africa.', '.'], dtype=object)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["income_df['country'].unique()\n","\n","#income_df['country'].nunique()"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["[('uk', 100),\n"," ('uk.', 100),\n"," ('uk/', 100),\n"," ('u.k.', 40),\n"," ('u.k/', 40),\n"," ('u.k', 40),\n"," ('united states/', 13),\n"," ('united states', 13),\n"," ('united states.', 13),\n"," ('united kingdom', 12)]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["# Libraries\n","import fuzzywuzzy\n","from fuzzywuzzy import process\n","import chardet\n","\n","# Set seed for reproducibility\n","np.random.seed(0)\n","\n","countries=income_df['country'].unique()\n","\n","# Get the top 10 closest matches to \"united kingdom\"\n","matches = fuzzywuzzy.process.extract(\"uk\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","# Inspect matches\n","matches\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["# Function to replace rows in the provided column of the provided DataFrame\n","# that match the provided string above the provided ratio with the provided string\n","\n","def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n","    # get a list of unique strings\n","    strings = df[column].unique()\n","    \n","    # Get the top 10 closest matches to our input string\n","    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n","                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","    # Only get matches with a ratio > 90\n","    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n","\n","    # Get the rows of all the close matches in our dataframe\n","    rows_with_matches = df[column].isin(close_matches)\n","\n","    # Replace all rows with close matches with the input matches \n","    df.loc[rows_with_matches, column] = string_to_match\n","    \n","    # Let us know when the function is done\n","    print(\"All done!\")"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["All done!\n","All done!\n","All done!\n","All done!\n","All done!\n","All done!\n","All done!\n","All done!\n"]},{"data":{"text/plain":["array(['united states', 'Britain', 'Britain/', 'united kingdom', 'U.K.',\n","       'sa', 'U.K/', 'america', nan, ' S.A.', 'england', 'uk', 'S.A./',\n","       'BRITAIN', 'U.K', 'U.K ', 'S.A. ', 'u.k', ' ', ' Britain',\n","       'united states of america', 'S.A..', 's.a.', ' U.K', 'Britain ',\n","       's. africasouth africa', ' S. AfricaSouth Africa',\n","       'S. AFRICASOUTH AFRICA', 'Britain.', '/', 'S. AfricaSouth Africa/',\n","       'S.A.', 'S. AfricaSouth Africa ', 'S. AfricaSouth Africa.',\n","       'S. AfricaSouth Africa', '.', 'britain'], dtype=object)"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["# Putting the function into use\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"united kingdom\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"united states\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"united states of america\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"south africa\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"uk\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"england\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"sa\")\n","replace_matches_in_column(df=income_df, column='country', string_to_match=\"america\")\n","\n","# Observing the unique countries after replacing the close matches\n","income_df['country'].nunique()\n","\n","income_df['country'].unique()"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["[('america', 100),\n"," ('america/', 100),\n"," ('america.', 100),\n"," ('united states of america', 45),\n"," ('united states of america.', 45),\n"," ('united states of america/', 45),\n"," ('britain', 43),\n"," ('britain/', 43),\n"," ('britain.', 43),\n"," ('s. africasouth africa', 37)]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# Get the top 10 closest matches to \"united kingdom\"\n","matches = fuzzywuzzy.process.extract(\"america\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n","\n","# Inspect matches\n","matches"]},{"cell_type":"markdown","metadata":{"id":"UJZDMTwP16Ws"},"source":["3. Create a new column called `days_ago` in the DataFrame that is a copy of the 'date_measured' column but instead it is a number that shows how many days ago it was measured from the current date. Note that the current date can be obtained using `datetime.date.today()`."]},{"cell_type":"code","execution_count":66,"metadata":{"id":"gMJbN84P16Wt"},"outputs":[],"source":["# Import modules\n","from datetime import date\n","\n","#Showing the column 'date_measured'\n","income_df['date_measured']\n","\n","#We can see that the date format is in days, month , year\n","#Therefore we will need to format our dates to %d-%m-%Y\n","\n","#creating a new column with the parsed dates\n","income_df['date_parsed'] = pd.to_datetime(income_df['date_measured'], format='%d-%m-%Y')\n","\n","income_df['date_parsed'].head()\n","\n","today=date.today()\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 ('phd')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"vscode":{"interpreter":{"hash":"63d17dc58a06b6a6d4136fb13c245dafcf53668da37b1c3052c24d689135f5bb"}}},"nbformat":4,"nbformat_minor":0}
